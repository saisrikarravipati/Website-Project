from pyspark.sql import functions as F

# 1) Map code -> description WITHOUT a UDF
tq_desc_map = {
    "TQ880": "Network Recon Validations",
    "TQ881": "Admin Fee Validations",
    "TQ882": "Bad Debt Validations",
    "TQ883": "Min Gtee Validations",
    "TQ884": "Network Access Fee Validations",
    "TQ885": "Specialty Cogs Validations",
}

# convert dict to a Spark map literal
mapping_expr = F.create_map(*[x for kv in tq_desc_map.items() for x in (F.lit(kv[0]), F.lit(kv[1]))])

# add TEST_DESC, keep or drop TQ_CD as you prefer
ce_regression_summary_daily_result = (
    ce_regression_summary_daily_result
    .withColumn("TEST_DESC", F.coalesce(mapping_expr[F.col("TQ_CD")], F.col("TQ_CD")))
    # .drop("TQ_CD")    # <-- uncomment only if you really want to remove it
)

# 2) Build the email table — make sure NOTHING references TQ_CD if you dropped it
COLUMNS = [
    "TEST_DESC", "TS_CTG", "PARENT_REQUEST_ID", "LOB", "APP_ID",
    "REQUEST_ID", "TASK_ID", "EXECUTION_STATUS", "TS_STATUS",
    "TOTAL_CNT", "SUCCESS_CNT", "FAILURE_CNT", "RUN_DATE_TS", "RUN_ID"
]

email_df = (ce_regression_summary_daily_result
            .select(*COLUMNS)
            .orderBy(F.col("RUN_ID").desc())   # DO NOT order by TQ_CD if it was dropped
            .limit(1000))

pdf = email_df.toPandas()
if "RUN_DATE_TS" in pdf.columns:
    pdf["RUN_DATE_TS"] = pdf["RUN_DATE_TS"].astype(str)
html_table = pdf.to_html(index=False, border=1, justify="center")
html_body = f"""
<html><body>
<h3>QAAP Daily Regression – Integrated Tests</h3>
<p>Rows: {len(pdf)}</p>
{html_table}
</body></html>
"""
csv_bytes = pdf.to_csv(index=False).encode("utf-8")

